use serde::{Deserialize, Serialize};

/// Represents API usage and cost data for a single scan
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct ScanCost {
    pub id: i64,
    pub scan_id: i64,
    pub files_analyzed_with_llm: i64,
    pub input_tokens: i64,
    pub output_tokens: i64,
    pub cache_read_tokens: i64,
    pub cache_write_tokens: i64,
    pub total_cost_usd: f64,
    pub created_at: String,
}

/// Claude API pricing (as of January 2025)
/// Source: https://www.anthropic.com/pricing#anthropic-api
pub struct ClaudePricing;

impl ClaudePricing {
    /// Haiku model pricing (used for LLM analysis)
    /// Input: $0.80 per million tokens
    pub const HAIKU_INPUT_PER_MILLION: f64 = 0.80;

    /// Output: $4.00 per million tokens
    pub const HAIKU_OUTPUT_PER_MILLION: f64 = 4.00;

    /// Cache read: $0.08 per million tokens (90% discount from input)
    pub const HAIKU_CACHE_READ_PER_MILLION: f64 = 0.08;

    /// Cache write: $1.00 per million tokens (25% surcharge on input)
    pub const HAIKU_CACHE_WRITE_PER_MILLION: f64 = 1.00;
}

impl ScanCost {
    /// Create a new ScanCost record from token usage
    ///
    /// # Arguments
    /// * `scan_id` - ID of the scan this cost belongs to
    /// * `files_analyzed_with_llm` - Number of files analyzed with LLM
    /// * `input_tokens` - Regular input tokens (no caching)
    /// * `output_tokens` - Output tokens generated by Claude
    /// * `cache_read_tokens` - Tokens read from prompt cache
    /// * `cache_write_tokens` - Tokens written to prompt cache
    ///
    /// # Returns
    /// ScanCost with calculated total_cost_usd
    pub fn new(
        scan_id: i64,
        files_analyzed_with_llm: i64,
        input_tokens: i64,
        output_tokens: i64,
        cache_read_tokens: i64,
        cache_write_tokens: i64,
    ) -> Self {
        let total_cost_usd = Self::calculate_cost(
            input_tokens,
            output_tokens,
            cache_read_tokens,
            cache_write_tokens,
        );

        Self {
            id: 0, // Will be set by database
            scan_id,
            files_analyzed_with_llm,
            input_tokens,
            output_tokens,
            cache_read_tokens,
            cache_write_tokens,
            total_cost_usd,
            created_at: chrono::Utc::now().to_rfc3339(),
        }
    }

    /// Calculate total USD cost from token usage using Claude Haiku pricing
    ///
    /// # Arguments
    /// * `input_tokens` - Regular input tokens
    /// * `output_tokens` - Output tokens
    /// * `cache_read_tokens` - Tokens read from cache
    /// * `cache_write_tokens` - Tokens written to cache
    ///
    /// # Returns
    /// Total cost in USD
    ///
    /// # Example
    /// ```
    /// use ryn::models::ScanCost;
    ///
    /// // 10,000 input tokens, 2,000 output tokens, no caching
    /// let cost = ScanCost::calculate_cost(10_000, 2_000, 0, 0);
    /// assert_eq!(cost, 0.016); // $0.008 input + $0.008 output
    /// ```
    pub fn calculate_cost(
        input_tokens: i64,
        output_tokens: i64,
        cache_read_tokens: i64,
        cache_write_tokens: i64,
    ) -> f64 {
        let input_cost = (input_tokens as f64 / 1_000_000.0) * ClaudePricing::HAIKU_INPUT_PER_MILLION;
        let output_cost = (output_tokens as f64 / 1_000_000.0) * ClaudePricing::HAIKU_OUTPUT_PER_MILLION;
        let cache_read_cost = (cache_read_tokens as f64 / 1_000_000.0) * ClaudePricing::HAIKU_CACHE_READ_PER_MILLION;
        let cache_write_cost = (cache_write_tokens as f64 / 1_000_000.0) * ClaudePricing::HAIKU_CACHE_WRITE_PER_MILLION;

        input_cost + output_cost + cache_read_cost + cache_write_cost
    }

    /// Get total tokens used (sum of all token types)
    pub fn total_tokens(&self) -> i64 {
        self.input_tokens + self.output_tokens + self.cache_read_tokens + self.cache_write_tokens
    }

    /// Get cost per file analyzed (0.0 if no files)
    pub fn cost_per_file(&self) -> f64 {
        if self.files_analyzed_with_llm == 0 {
            0.0
        } else {
            self.total_cost_usd / self.files_analyzed_with_llm as f64
        }
    }

    /// Get average tokens per file (0 if no files)
    pub fn avg_tokens_per_file(&self) -> f64 {
        if self.files_analyzed_with_llm == 0 {
            0.0
        } else {
            self.total_tokens() as f64 / self.files_analyzed_with_llm as f64
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_calculate_cost_no_tokens() {
        let cost = ScanCost::calculate_cost(0, 0, 0, 0);
        assert_eq!(cost, 0.0);
    }

    #[test]
    fn test_calculate_cost_input_only() {
        // 1 million input tokens = $0.80
        let cost = ScanCost::calculate_cost(1_000_000, 0, 0, 0);
        assert_eq!(cost, 0.80);
    }

    #[test]
    fn test_calculate_cost_output_only() {
        // 1 million output tokens = $4.00
        let cost = ScanCost::calculate_cost(0, 1_000_000, 0, 0);
        assert_eq!(cost, 4.00);
    }

    #[test]
    fn test_calculate_cost_cache_read_only() {
        // 1 million cache read tokens = $0.08
        let cost = ScanCost::calculate_cost(0, 0, 1_000_000, 0);
        assert_eq!(cost, 0.08);
    }

    #[test]
    fn test_calculate_cost_cache_write_only() {
        // 1 million cache write tokens = $1.00
        let cost = ScanCost::calculate_cost(0, 0, 0, 1_000_000);
        assert_eq!(cost, 1.00);
    }

    #[test]
    fn test_calculate_cost_mixed_tokens() {
        // 10,000 input ($0.008) + 2,000 output ($0.008) + 5,000 cache read ($0.0004) + 3,000 cache write ($0.003)
        let cost = ScanCost::calculate_cost(10_000, 2_000, 5_000, 3_000);
        let expected = (10_000.0 / 1_000_000.0) * 0.80
            + (2_000.0 / 1_000_000.0) * 4.00
            + (5_000.0 / 1_000_000.0) * 0.08
            + (3_000.0 / 1_000_000.0) * 1.00;
        assert!((cost - expected).abs() < 0.000001); // Float comparison
    }

    #[test]
    fn test_calculate_cost_realistic_scan() {
        // Realistic scan: 50 files, ~200k input tokens, ~50k output tokens
        // With prompt caching: 150k cache reads, 50k cache writes
        let cost = ScanCost::calculate_cost(50_000, 50_000, 150_000, 50_000);

        // Expected:
        // Input: 50k * $0.80/1M = $0.04
        // Output: 50k * $4.00/1M = $0.20
        // Cache read: 150k * $0.08/1M = $0.012
        // Cache write: 50k * $1.00/1M = $0.05
        // Total: $0.302
        let expected = 0.04 + 0.20 + 0.012 + 0.05;
        assert!((cost - expected).abs() < 0.001);
    }

    #[test]
    fn test_scan_cost_new() {
        let scan_cost = ScanCost::new(123, 50, 10_000, 2_000, 5_000, 3_000);

        assert_eq!(scan_cost.id, 0); // Not yet saved to DB
        assert_eq!(scan_cost.scan_id, 123);
        assert_eq!(scan_cost.files_analyzed_with_llm, 50);
        assert_eq!(scan_cost.input_tokens, 10_000);
        assert_eq!(scan_cost.output_tokens, 2_000);
        assert_eq!(scan_cost.cache_read_tokens, 5_000);
        assert_eq!(scan_cost.cache_write_tokens, 3_000);
        assert!(scan_cost.total_cost_usd > 0.0);
        assert!(!scan_cost.created_at.is_empty());
    }

    #[test]
    fn test_scan_cost_new_calculates_cost_correctly() {
        let scan_cost = ScanCost::new(1, 10, 10_000, 2_000, 0, 0);

        let expected_cost = ScanCost::calculate_cost(10_000, 2_000, 0, 0);
        assert_eq!(scan_cost.total_cost_usd, expected_cost);
    }

    #[test]
    fn test_total_tokens() {
        let scan_cost = ScanCost::new(1, 10, 1_000, 2_000, 3_000, 4_000);
        assert_eq!(scan_cost.total_tokens(), 10_000);
    }

    #[test]
    fn test_total_tokens_zero() {
        let scan_cost = ScanCost::new(1, 0, 0, 0, 0, 0);
        assert_eq!(scan_cost.total_tokens(), 0);
    }

    #[test]
    fn test_cost_per_file() {
        let scan_cost = ScanCost::new(1, 10, 10_000, 2_000, 0, 0);
        let expected_cost_per_file = scan_cost.total_cost_usd / 10.0;
        assert!((scan_cost.cost_per_file() - expected_cost_per_file).abs() < 0.000001);
    }

    #[test]
    fn test_cost_per_file_zero_files() {
        let scan_cost = ScanCost::new(1, 0, 10_000, 2_000, 0, 0);
        assert_eq!(scan_cost.cost_per_file(), 0.0);
    }

    #[test]
    fn test_avg_tokens_per_file() {
        let scan_cost = ScanCost::new(1, 5, 10_000, 5_000, 0, 0);
        assert_eq!(scan_cost.avg_tokens_per_file(), 3_000.0); // 15,000 total / 5 files
    }

    #[test]
    fn test_avg_tokens_per_file_zero_files() {
        let scan_cost = ScanCost::new(1, 0, 10_000, 2_000, 0, 0);
        assert_eq!(scan_cost.avg_tokens_per_file(), 0.0);
    }

    #[test]
    fn test_scan_cost_serde() {
        let scan_cost = ScanCost::new(1, 10, 10_000, 2_000, 5_000, 3_000);

        let json = serde_json::to_string(&scan_cost).unwrap();
        let deserialized: ScanCost = serde_json::from_str(&json).unwrap();

        assert_eq!(scan_cost, deserialized);
    }

    #[test]
    fn test_scan_cost_clone() {
        let scan_cost = ScanCost::new(1, 10, 10_000, 2_000, 0, 0);
        let cloned = scan_cost.clone();

        assert_eq!(scan_cost, cloned);
    }

    #[test]
    fn test_claude_pricing_constants() {
        // Verify pricing constants are set correctly (January 2025)
        assert_eq!(ClaudePricing::HAIKU_INPUT_PER_MILLION, 0.80);
        assert_eq!(ClaudePricing::HAIKU_OUTPUT_PER_MILLION, 4.00);
        assert_eq!(ClaudePricing::HAIKU_CACHE_READ_PER_MILLION, 0.08);
        assert_eq!(ClaudePricing::HAIKU_CACHE_WRITE_PER_MILLION, 1.00);
    }

    #[test]
    fn test_cost_calculation_matches_anthropic_pricing() {
        // Test case from Anthropic docs: 1M input + 1M output with caching
        // Input: 500k regular + 500k cache read
        // Output: 1M tokens
        // Cache write: 100k tokens

        let cost = ScanCost::calculate_cost(
            500_000,   // Regular input
            1_000_000, // Output
            500_000,   // Cache read
            100_000,   // Cache write
        );

        // Expected:
        // Regular input: 500k * $0.80/1M = $0.40
        // Output: 1M * $4.00/1M = $4.00
        // Cache read: 500k * $0.08/1M = $0.04
        // Cache write: 100k * $1.00/1M = $0.10
        // Total: $4.54

        let expected = 0.40 + 4.00 + 0.04 + 0.10;
        assert!((cost - expected).abs() < 0.001);
    }

    #[test]
    fn test_zero_cost_for_zero_tokens() {
        let scan_cost = ScanCost::new(1, 100, 0, 0, 0, 0);
        assert_eq!(scan_cost.total_cost_usd, 0.0);
        assert_eq!(scan_cost.cost_per_file(), 0.0);
    }

    #[test]
    fn test_realistic_scan_metrics() {
        // Realistic scenario: Smart mode scan of 40 files
        // ~15k tokens per file average (input + output + cache)
        let scan_cost = ScanCost::new(
            1,      // scan_id
            40,     // files analyzed
            200_000, // input tokens
            80_000,  // output tokens
            300_000, // cache read (reusing SOC2 controls prompt)
            50_000,  // cache write
        );

        assert_eq!(scan_cost.files_analyzed_with_llm, 40);
        assert_eq!(scan_cost.total_tokens(), 630_000);
        assert!(scan_cost.avg_tokens_per_file() > 15_000.0);
        assert!(scan_cost.avg_tokens_per_file() < 16_000.0);

        // Cost should be under $1 for this realistic scan
        assert!(scan_cost.total_cost_usd < 1.0);
        assert!(scan_cost.total_cost_usd > 0.0);

        // Cost per file should be a few cents
        assert!(scan_cost.cost_per_file() < 0.10);
        assert!(scan_cost.cost_per_file() > 0.0);
    }
}
